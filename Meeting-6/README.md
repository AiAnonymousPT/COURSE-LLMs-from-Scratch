# Meeting 6: Fine-Tuning LLMs

6 months into building LLMs! We're almost there, our LLM is taking shape.

*Mandatory materials are required for everyone to complete before each meeting.*

Optional materials explore the state-of-the-art of LLMs training & fine-tuning.

Please schedule your time accordingly.

## Materials

- *Mandatory*:
  - Chapter 6: Fine-tuning for classification
  - Chapter 7: Fine-tuning to follow instructions
  - Appendix E: Parameter-efficient fine-tuning with LoRA

<br>

- *Optional*:
  - Overview on SoTA: [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196)
  - From GPT to GPT4: [Fine-tuning LLMs](https://synthesis.ai/2024/08/13/fine-tuning-llms-rlhf-lora-and-instruction-tuning/)
  - Alignment on top of fine-tuning: [A Survey Of Alignment Techniques](https://arxiv.org/pdf/2407.16216v1)
  
<br>

- *Estimated workload*:
  - mandatory: 40h
  - optional: 5h

&nbsp;

## Meeting Agenda
- Summary, code review
- Group discussion: fine-tuning challenges
- Q&A, debugging