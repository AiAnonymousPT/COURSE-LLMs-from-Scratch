
<img src="Supporting/Images/course_logo.png" alt="LLMs From Scratch" width="50%">

## Course Overview
- Do you use LLMs as part of your work flow but have yet to dive into the interworkings of these models? 
- Are you looking for a structured learning approach combined with a peer support group? 

Same for us - that's why we created this course.

"What I cannot create, I do not understand." Richard P. Feynman


### Objectives
The goal of this course is that each participant leaves with a granular understanding and ability to manipulate LLMs from architecture design to fine-tuning. We view this as a foundational building block for further research into advanced topics and/or building applications with LLMs.

### Format
- There is no teacher for this course, rather this is a group study of carefully selected materials. 
- We will meet once a month in a casual interactive setting to discuss the materials, answer questions, and work on exercises together. We will also use Discourse to keep the conversation going in real-time.

### Book [required]
We will be working through a number of resources, most of which are freely available online. However we will be relying heavily on the following required text: 
- "Build a Large Language Model (From Scratch)" by Sebastian Raschka
- Available as PDF, EPUB https://www.manning.com/books/build-a-large-language-model-from-scratch
- Discount code: SRLLM24
- Full code repo: https://github.com/rasbt/LLMs-from-scratch

### Activites
- Study: read/watch materials, run the code, and solve the exercises
- Research: deliberate practice, debug, try to break things
- Group meeting: once every 4 weeks to summarize insights, review code, Q&A
- Discord: discuss about anything, share resources, ask questions

<br>

## Schedule & Topics

We plan to meet every 3/4 weeks, around the last week of the month.
- Meeting 0 [Getting started](Meeting-0/README.md) - Mon, Nov 4
- Meeting 1 [Tokenization & Embeddings](Meeting-1/README.md) - Mon, Nov 25
- Meeting 2 [Project: build your own tokenizer](Meeting-2/README.md) - Mon, Jan 6
- Meeting 3 [Attention Mechanisms](Meeting-3/README.md) - Mon, Jan 27
- Meeting 4 [Transformer & GPT Architecture](Meeting-4/README.md) - Mon, Feb 24
- Meeting 5 [Pretraining LLMs](Meeting-5/README.md) - Mon, Mar 24
- Meeting 6 [Fine-tuning LLMs](Meeting-6/README.md) - Mon, Apr 21
- Meeting 7 [Final Project: build your own GPT-2](Meeting-7/README.md) - Mon, May 26

The first 3 months have optional materials allowing for everyone to acquire the fundamentals.

<br>

### Milestones
1) Build a working BPE tokenizer from scratch
2) Build your Transformer architecture with multi-head self-attention
3) Pre-train and fine-tune an LLM, based on your own dataset and hardware.
4) Build your own GPT-2, fine-tune it for your usecase 