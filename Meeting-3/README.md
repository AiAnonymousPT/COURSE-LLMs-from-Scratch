# Meeting 3: Attention Mechanisms

3 months into building LLMs already! Attention is all you need. It's at the heart of Transformers and LLMs.

*Mandatory materials are required for everyone to complete before each meeting.*

Optional materials dive deeper and visualize the inner workings of Attention Mechanisms.

Please schedule your time accordingly.

## Materials

- *Mandatory*:
  - Chapter 3: Attention mechanisms
  - "Attention Is All You Need" [paper explained](https://www.oxen.ai/blog/arxiv-dives-attention-is-all-you-need)
  - Stanford CS224N|2023 Lecture: [Self-Attention and Transformers](https://www.youtube.com/watch?v=LWMzyfvuehA&list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&index=9)  


<br>

- *Optional*:
  - 3Blue1Brown: [Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc&ab_channel=3Blue1Brown)
  - More on Attention: [How Mistral 7b Works](https://www.oxen.ai/blog/arxiv-dive-how-to-mistral-7b-works)
  - Visualizing Attention with [Bertviz](https://github.com/jessevig/bertviz?tab=readme-ov-file#-quick-tour)

<br>

- *Estimated workload*:
  - mandatory: 30h
  - optional: 10h

&nbsp;

## Meeting Agenda
- Summary, code review
- Group discussion: pros and cons of Attention
- Q&A, debugging