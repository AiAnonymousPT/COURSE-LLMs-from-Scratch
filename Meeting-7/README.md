# Meeting 7: Final Project - Build your own LLM

This is it! Your own LLM is up and running.

In this wrap-up meeting, we will showcase our LLMs, discuss the challenges, and share insights.

Furthermore, we're now prepared for the next level: RAG, quantization, and more!

## Materials

- *Mandatory*:
  - Resources: [Let's Reproduce GPT2 by Kaparthy](https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10)
  - Datasets hub: [Hugging Face Datasets](https://huggingface.co/datasets)
  - Instructions all-in: [Instruction Pretraining LLMs](https://www.linkedin.com/pulse/instruction-pretraining-llms-sebastian-raschka-phd-x6zoc/)

<br>

- *Optional*:
  - Full circle: where it started [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  - Meta's Llama origin: [OPT: Open Pre-trained Transformers](https://arxiv.org/pdf/2205.01068) 

<br>

- *Estimated workload*:
  - mandatory: 40h
  - optional: 5h

<br>

## Meeting Agenda
- Showcase your LLM
- Group discussion: concluding thoughts, share insights
- Q&A